It's an idea I've been thinking about lately: voxel engines. I was inspired by a YouTuber named TodePond who explores voxel engines in the context of cellular automata. The idea is to have a 3D grid of voxels and a set of "elements" with rules governing how a given region of space (e.g., 3x3x3) transforms into a new state. This framework allows us to simulate elements resembling real-world phenomena such as falling and sliding sand, water, growing plants, and even traveling beings.

Another inspiration for me is Minecraft. I've played it a bit and enjoy the ability to modify the surroundings, create complex buildings, and explore landscapes. However, both sources of inspiration have a major problem: computational efficiency. Minecraft struggles to render vast landscapes because the number of voxels overwhelms consumer-grade GPUs. Similarly, simulating large-scale cellular automata becomes computationally intensive due to the need to check each voxel.

The first concept I had, some time ago, was to simplify the rendering problem by making it easily cacheable. Instead of a full 3D rendering pipeline, we can have a 2.5D engine using "parallax rendering." When we view the scene along an axis, we divide it into increasingly deeper planes (possibly exponentially increasing depths). Each plane is then rendered flat into a 2D image. By composing and slightly shifting these planes as the observer moves, we can create an illusion of depth. Pre-rendered images can be cached for later use. If a plane hasn't changed, we can reuse it. Additionally, plan renderings can be shared among many players. Even if voxels have changed, we can set a threshold for re-rendering a plane only if there has been a significant change. This is because changes in a single voxel may not be visible from a distance or may be hidden by other voxels.

The second concept aims to optimize cellular automata by precomputing transitions for longer periods than a single tick and applying these aggregated transitions when nobody is near the affected area. Aggregating by space alone is not very useful; although we apply fewer transitions, each transition has more possibilities, making it more computationally intensive to select a matching transition. Aggregating by time, on the other hand, is more beneficial. With a single transition, we can cover multiple time steps. However, to maintain the speed of information propagation, we need to increase the spatial size of the transition. For example, imagine a voxel with a rule that moves it one space to the left each tick.

For larger aggregated rules (e.g., size 128), it may be impossible to precompute all transitions due to the sheer number of possibilities. In this case, we can employ a predictive model to approximate the transitions. Although this model won't be 100% accurate, it will be sufficient to create an illusion of time passing. Players won't be able to discern the inaccuracies because the model will only be used when nobody is near the area in question. This approach is analogous to describing the world at a different level of detail. Instead of simulating every piece of wood and every leaf, we can simply say that the tree is growing.

Q: How do you plan on designing the predictive model? Will it be based on certain patterns or behaviors observed in smaller rule sets, or will it incorporate some form of machine learning?
A: The predictive model will be based on machine learning (ML). I don't want the higher-level rules to be written by hand, but rather automatically generated from the base (low-level) rules. One advantage is that we don't have to worry about training data since we can simulate as much as we want. However, it's unclear whether we should train on random voxel structures or on voxel structures that occur in the world. I believe the former approach would be more suitable. Therefore, the argument about not worrying about training data may not be completely true.

Q: The system's performance would probably depend on the distribution of players across the voxel world. How will the system adapt if many players are spread out, requiring more detailed simulation in various areas?
A: We don't need to worry about per-player computation. Each player will require some level of compute, and that is unavoidable. However, we can work on reducing the per-voxel complexity to improve performance.

Q: Will there be a limit to how much the world can be abstracted? For instance, would the growth of a tree still be represented as a single tree growing larger, or could it eventually be abstracted to "a forest is growing"?
A: We can create aggregated rules on many different levels. For example, we can have rules exponentially increasing in size (e.g., rules for t=128, t=256, and so on). Each of these models may capture processes relevant for a given level of detail.

Q: How might this system handle actions that have long-term, world-altering effects? For example, if a player sets a forest on fire, how would the predictive model simulate the spread of the fire when no players are around?
A: Simulating a burning forest is not significantly different from simulating a growing forest. Both processes can be described at a given level of detail. However, training a predictive model to capture these rare, human-induced processes might be challenging due to their infrequency in the training data.